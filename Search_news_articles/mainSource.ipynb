{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어를 입력해주세요. : 데이터분석\n",
      "검색 시작 날짜를 입력해주세요. (형식 : 2022.01.01) : \n",
      "검색 종료 날짜를 입력해주세요. (형식 : 2022.12.31) : \n",
      "정렬 타입을 입력해주세요. (관련도순 = 0, 최신순 = 1, 오래된순 = 2) : 0\n",
      "크롤링을 원하는 페이지 수를 입력해주세요. : 2\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n"
     ]
    }
   ],
   "source": [
    "query = input('검색어를 입력해주세요. : ')\n",
    "start_date = input('검색 시작 날짜를 입력해주세요. (형식 : 2022.01.01) : ')\n",
    "end_date = input('검색 종료 날짜를 입력해주세요. (형식 : 2022.12.31) : ')\n",
    "sort_type = input('정렬 타입을 입력해주세요. (관련도순 = 0, 최신순 = 1, 오래된순 = 2) : ')\n",
    "max_page = input('크롤링을 원하는 페이지 수를 입력해주세요. : ')\n",
    "\n",
    "if start_date > end_date :\n",
    "    print('\\n시작 날짜는 종료 날짜보다 이후로 지정 불가합니다.')\n",
    "elif max_page == '':\n",
    "    max_page = 5\n",
    "    print('\\n원하는 페이지 수가 입력되지 않았습니다. 5페이지 까지 크롤링 진행')\n",
    "    main_crawling(query, start_date, end_date, sort_type, max_page)\n",
    "else:\n",
    "    max_page = int(max_page)\n",
    "    main_crawling(query, start_date, end_date, sort_type, max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from bs4 import BeautifulSoup # HTTP Response ->   HTML\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_crawling(query, start_date, end_date, sort_type, max_page):\n",
    "    if query == '':\n",
    "        query = '데이터 분석'\n",
    "    if len(start_date) != 10:\n",
    "        start_date = '2022.01.01'\n",
    "    if len(end_date) != 10:\n",
    "        end_date = '2022.12.31'\n",
    "    if sort_type not in ['0','1','2']:\n",
    "        sort_type = '0'\n",
    "    \n",
    "    # 각 기사들을 담을 리스트 생성\n",
    "    titles = []\n",
    "    dates = []\n",
    "    articles = []\n",
    "    article_urls = []\n",
    "    press_companies = []\n",
    "    \n",
    "    # 주어진 일자를 쿼리에 맞는 형태로 변경\n",
    "    start_date = start_date.replace(\".\",\"\")\n",
    "    end_date = end_date.replace(\".\",\"\")    \n",
    "    \n",
    "    # 지정한 기간 내 원하는 페이지 수만큼의 기사를 크롤링\n",
    "    current_call = 1\n",
    "    last_call = (max_page - 1) * 10 + 1 # max_page이 5일 경우 41에 해당\n",
    "    # 1 11 21 31 41....\n",
    "    \n",
    "    while current_call <= last_call:\n",
    "        print('\\n{}번째 기사글부터 크롤링을 시작합니다.'.format(current_call))\n",
    "        \n",
    "        url = \"https://search.naver.com/search.naver?where=news&query=\" + query \\\n",
    "        + \"&sort\" + sort_type \\\n",
    "        + \"nso=so%3Ar%2Cp%3Afrom\" + start_date \\\n",
    "        + \"to\" + end_date \\\n",
    "        + \"%2Ca%3A&start=\" + str(current_call)\n",
    "        \n",
    "        urls_list = []\n",
    "        \n",
    "        try:\n",
    "            web = requests.get(url).content\n",
    "            source = BeautifulSoup(web,'html.parser')\n",
    "            for urls in source.find_all('a',{'class':'info'}):\n",
    "                if urls[\"href\"].startswith(\"https://n.news.naver.com\"):\n",
    "                urls_list.append(urls[\"href\"])\n",
    "        except:\n",
    "            print(\"해당 뉴스 검색 페이지의 뉴스 링크를 모으는 중 에러 발생: \", url)\n",
    "        \n",
    "        \n",
    "        if urls_list != []:\n",
    "            for url in urls_list:\n",
    "                try:\n",
    "                   \n",
    "                    headers = {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.101 Safari/537.36where: newssm: tab_jumquery: ' + quote('데이터분석')}\n",
    "                    web_news = requests.get(url, headers=headers).content\n",
    "                    source_news = BeautifulSoup(web_news,'html.parser')\n",
    "                    #print(url)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

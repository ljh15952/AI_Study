{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어를 입력해주세요. : 데이터 분석\n",
      "검색 시작 날짜를 입력해주세요. (형식 : 2022.01.01) : \n",
      "검색 종료 날짜를 입력해주세요. (형식 : 2022.12.31) : \n",
      "정렬 타입을 입력해주세요. (관련도순 = 0, 최신순 = 1, 오래된순 = 2) : 1\n",
      "크롤링을 원하는 페이지 수를 입력해주세요. : 3\n",
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "Processing article : ADD \"'열악한 환경' 속 자율주행 위한 데이터 확보\"\n",
      "Processing article : 포항TP 경북SW진흥본부, 2022 동북권 공공데이터 챌린지 참가자 모집\n",
      "Processing article : 특허청, 5억만건 특허빅데이터 분석 '6대 전략기술' 맞춤 지원\n",
      "Processing article : 부산 빅데이터 혁신센터 개소…데이터 관련 사업화 지원\n",
      "Processing article : 서울 1인 가구 빅데이터 분석해보니…노년층 고립 위험 커\n",
      "Processing article : 통계청, '데이터 활용 대회' 대상에 작물 변화 분석 보고서\n",
      "Processing article : 이화여대, 자기소개서 폐지…데이터사이언스학과 신설\n",
      "\n",
      "11번째 기사글부터 크롤링을 시작합니다.\n",
      "Processing article : 포항TP 경북SW진흥본부, 2022 동북권 공공데이터 챌린지 참가자 모집\n",
      "Processing article : 전남대 연구팀 '한우 무게 측정모델' 개발…3차원카메라·빅데이터 활용\n",
      "Processing article : 국토부, 공간빅데이터 경진대회…지역현안 해결 자료로 활용\n",
      "Processing article : 부산산업과학혁신원, 부산연구개발사업 조사·분석 시행\n",
      "Processing article : 이화여대, 자기소개서 폐지…데이터사이언스학과 신설\n",
      "Processing article : 통계청, '데이터 활용 대회' 대상에 작물 변화 분석 보고서\n",
      "Processing article : 충남 데이터 포털 '올담' 구축…29일부터 서비스\n",
      "Processing article : AI·빅데이터 등 디지털신기술 탑재한 차세대 나라장터, 첫 선\n",
      "Processing article : 비즈데이터, 2022 브랜드파워대상 '환경 AI 빅데이터'부문 대상 수상\n",
      "\n",
      "21번째 기사글부터 크롤링을 시작합니다.\n",
      "Processing article : 데이터로 찾아낸 '저 연차가 일하기 좋은 기업' 톱10\n",
      "Processing article : 코드에프 \"데이터 중개 플랫폼 '종횡무진'…마켓플레이스 시장 선도\"\n",
      "Processing article : 10년간 축적해둔 공공데이터…민간에 풀면 혁신 불붙어\n",
      "Processing article : 이화여대 의과대학 학부생 연구팀, 아시아 지역의 신경계 질환 질병부담의 차이 분석\n",
      "Processing article : 복지부 예산안, 의료데이터·디지털헬스케어 ‘뜨고’ 제약·바이오 ‘주춤’\n"
     ]
    }
   ],
   "source": [
    "query = input('검색어를 입력해주세요. : ')\n",
    "start_date = input('검색 시작 날짜를 입력해주세요. (형식 : 2022.01.01) : ')\n",
    "end_date = input('검색 종료 날짜를 입력해주세요. (형식 : 2022.12.31) : ')\n",
    "sort_type = input('정렬 타입을 입력해주세요. (관련도순 = 0, 최신순 = 1, 오래된순 = 2) : ')\n",
    "max_page = input('크롤링을 원하는 페이지 수를 입력해주세요. : ')\n",
    "\n",
    "if start_date > end_date :\n",
    "    print('\\n시작 날짜는 종료 날짜보다 이후로 지정 불가합니다.')\n",
    "elif max_page == '':\n",
    "    max_page = 5\n",
    "    print('\\n원하는 페이지 수가 입력되지 않았습니다. 5페이지 까지 크롤링 진행')\n",
    "    main_crawling(query, start_date, end_date, sort_type, max_page)\n",
    "else:\n",
    "    max_page = int(max_page)\n",
    "    main_crawling(query, start_date, end_date, sort_type, max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from bs4 import BeautifulSoup # HTTP Response ->   HTML\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_crawling(query, start_date, end_date, sort_type, max_page):\n",
    "    if query == '':\n",
    "        query = '데이터 분석'\n",
    "    if len(start_date) != 10:\n",
    "        start_date = '2022.01.01'\n",
    "    if len(end_date) != 10:\n",
    "        end_date = '2022.12.31'\n",
    "    if sort_type not in ['0','1','2']:\n",
    "        sort_type = '0'\n",
    "    \n",
    "    # 각 기사들을 담을 리스트 생성\n",
    "    titles = []\n",
    "    dates = []\n",
    "    articles = []\n",
    "    article_urls = []\n",
    "    press_companies = []\n",
    "    \n",
    "    # 주어진 일자를 쿼리에 맞는 형태로 변경\n",
    "    start_date = start_date.replace(\".\",\"\")\n",
    "    end_date = end_date.replace(\".\",\"\")    \n",
    "    \n",
    "    # 지정한 기간 내 원하는 페이지 수만큼의 기사를 크롤링\n",
    "    current_call = 1\n",
    "    last_call = (max_page - 1) * 10 + 1 # max_page이 5일 경우 41에 해당\n",
    "    # 1 11 21 31 41....\n",
    "    \n",
    "    while current_call <= last_call:\n",
    "        print('\\n{}번째 기사글부터 크롤링을 시작합니다.'.format(current_call))\n",
    "        \n",
    "        url = \"https://search.naver.com/search.naver?where=news&query=\" + query \\\n",
    "        + \"&sort\" + sort_type \\\n",
    "        + \"nso=so%3Ar%2Cp%3Afrom\" + start_date \\\n",
    "        + \"to\" + end_date \\\n",
    "        + \"%2Ca%3A&start=\" + str(current_call)\n",
    "        \n",
    "        urls_list = []\n",
    "        \n",
    "        try:\n",
    "            web = requests.get(url).content\n",
    "            source = BeautifulSoup(web,'html.parser')\n",
    "            for urls in source.find_all('a',{'class':'info'}):\n",
    "                if urls[\"href\"].startswith(\"https://n.news.naver.com\"):\n",
    "                    urls_list.append(urls[\"href\"])\n",
    "        except:\n",
    "            print(\"해당 뉴스 검색 페이지의 뉴스 링크를 모으는 중 에러 발생: \", url)\n",
    "        \n",
    "        \n",
    "        if urls_list != []:\n",
    "            for url in urls_list:\n",
    "                try:\n",
    "                    headers = {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.101 Safari/537.36where: newssm: tab_jumquery: ' + quote('데이터분석')}\n",
    "                    web_news = requests.get(url, headers=headers).content\n",
    "                    source_news = BeautifulSoup(web_news,'html.parser')\n",
    "                    # 2) 기사 제목\n",
    "                    title = source_news.find('h2', {'class':'media_end_head_headline'}).get_text()\n",
    "                    print('Processing article : {}'.format(title))\n",
    "\n",
    "                    # 3) 기사 날짜\n",
    "                    date = source_news.find('span',{'class':'media_end_head_info_datestamp_time'}).get_text()\n",
    "                    #print('Date of article : {}'.format(date))\n",
    "\n",
    "                    # 4) 기사 본문\n",
    "                    article = source_news.find('div',{'id':'dic_area'}).get_text()\n",
    "                    article = article.replace(\"\\n\",\"\")\n",
    "                    article = article.replace(\"\\t\",\"\")\n",
    "\n",
    "                    # 5) 기사 발행 언론사\n",
    "                    press_company = source_news.find('em',{'class':'media_end_linked_more_point'}).get_text()\n",
    "\n",
    "                    # 위 2~5를 통해 성공적으로 제목/날짜/본문/언론사 정보가 모두 추출되었을 때에만 리스트에 추가해 길이를 동일하게 유지\n",
    "                    titles.append(title)\n",
    "                    dates.append(date)\n",
    "                    articles.append(article)\n",
    "                    press_companies.append(press_company)\n",
    "                    article_urls.append(url)\n",
    "                except:\n",
    "                    print(\"다음 뉴스기사의 링크를 스크랩 중 오류가 발생하였음 : {}\".format(url))\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        time.sleep(5)\n",
    "        current_call += 10\n",
    "        \n",
    "    article_df = pd.DataFrame({'Title':titles,\n",
    "                            'Data':dates,\n",
    "                          'Article':articles,\n",
    "                          'URL':article_urls,\n",
    "                          'PressCompany':press_companies})\n",
    "    from openpyxl import Workbook\n",
    "    article_df.to_excel('result2_{}.xlsx'.format(datetime.now().strftime('%y%m%d_%H%M')),index=False,encoding='utf-8')\n",
    "    article_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

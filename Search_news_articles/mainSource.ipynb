{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1번째 기사글부터 크롤링을 시작합니다.\n",
      "Processing article : [속보] 경찰 기동대, 이태원 사고 85분 뒤에서야 첫 현장 도착\n",
      "Processing article : [속보] '이태원 참사' 행안위에 오세훈·김광호·박희영 증인 출석\n",
      "Processing article : 尹, 이태원 참사 희생자 추모미사 참석…김건희 여사 동행\n",
      "Processing article : [이태원 참사] 29분 뒤 형사인력 투입…마약단속은 '0건'\n",
      "Processing article : [속보]尹대통령·김건희 여사, 이태원 참사 추모미사 참석\n",
      "Processing article : [속보] 윤 대통령 내외 이태원 추모미사 묵상 기도\n",
      "Processing article : [단독]이태원 참사 '응급의료 컨트롤타워', 결원도 못 채울 판\n",
      "Processing article : 이태원 참사 '토끼 머리띠' 남성 \"얼굴 공개한 사람들 다 고소\"\n",
      "Processing article : [속보] 행안위 \"내일 오세훈·김광호·박희영 '이태원 참사' 증인 출석\"\n",
      "Processing article : 이태원 참사 미사 참석한 윤석열 대통령 내외 [포착]\n",
      "\n",
      "11번째 기사글부터 크롤링을 시작합니다.\n",
      "Processing article : \"윤석열 퇴진이 이태원 추모\" 참사 후 첫 촛불집회\n",
      "Processing article : [속보]尹대통령·김건희 여사, 이태원 참사 추모미사 참석\n",
      "Processing article : [이태원 참사] 용산서장, '차량이동' 고집해 현장 50분 늦게 도착\n",
      "Processing article : 이태원 참사 내국인 사망자 130명 장례…오늘 마무리(종합)\n",
      "Processing article : 이태원 참사 미사 참석한 윤석열 대통령 내외 [포착]\n",
      "Processing article : 행안위, 내일 이태원 참사 질의…오세훈·김광호·박희영 참석\n",
      "Processing article : 이태원 참사 후 尹대통령 지지율 추이 보니\n",
      "Processing article : [이태원 참사] 행안부 \"서울시·용산구에 오후 10시53분 상황관리 지시\"\n",
      "Processing article : [포착] 다시 켜진 촛불…한마음으로 이태원 참사 추모\n",
      "Processing article : [이태원 참사] 尹대통령, 엿새째 조문…한덕수·이상민 등 내각 동행\n"
     ]
    }
   ],
   "source": [
    "query = input('검색어를 입력해주세요. : ')\n",
    "start_date = input('검색 시작 날짜를 입력해주세요. (형식 : 2022.01.01) : ')\n",
    "end_date = input('검색 종료 날짜를 입력해주세요. (형식 : 2022.12.31) : ')\n",
    "sort_type = input('정렬 타입을 입력해주세요. (관련도순 = 0, 최신순 = 1, 오래된순 = 2) : ')\n",
    "max_page = input('크롤링을 원하는 페이지 수를 입력해주세요. : ')\n",
    "\n",
    "if start_date > end_date :\n",
    "    print('\\n시작 날짜는 종료 날짜보다 이후로 지정 불가합니다.')\n",
    "elif max_page == '':\n",
    "    max_page = 5\n",
    "    print('\\n원하는 페이지 수가 입력되지 않았습니다. 5페이지 까지 크롤링 진행')\n",
    "    main_crawling(query, start_date, end_date, sort_type, max_page)\n",
    "else:\n",
    "    max_page = int(max_page)\n",
    "    main_crawling(query, start_date, end_date, sort_type, max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from bs4 import BeautifulSoup # HTTP Response ->   HTML\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_crawling(query, start_date, end_date, sort_type, max_page):\n",
    "    if query == '':\n",
    "        query = '데이터 분석'\n",
    "    if len(start_date) != 10:\n",
    "        start_date = '2022.01.01'\n",
    "    if len(end_date) != 10:\n",
    "        end_date = '2022.12.31'\n",
    "    if sort_type not in ['0','1','2']:\n",
    "        sort_type = '0'\n",
    "    \n",
    "    # 각 기사들을 담을 리스트 생성\n",
    "    titles = []\n",
    "    dates = []\n",
    "    articles = []\n",
    "    article_urls = []\n",
    "    press_companies = []\n",
    "    \n",
    "    # 주어진 일자를 쿼리에 맞는 형태로 변경\n",
    "    start_date = start_date.replace(\".\",\"\")\n",
    "    end_date = end_date.replace(\".\",\"\")    \n",
    "    \n",
    "    # 지정한 기간 내 원하는 페이지 수만큼의 기사를 크롤링\n",
    "    current_call = 1\n",
    "    last_call = (max_page - 1) * 10 + 1 # max_page이 5일 경우 41에 해당\n",
    "    # 1 11 21 31 41....\n",
    "    \n",
    "    while current_call <= last_call:\n",
    "        print('\\n{}번째 기사글부터 크롤링을 시작합니다.'.format(current_call))\n",
    "        \n",
    "        url = \"https://search.naver.com/search.naver?where=news&query=\" + query \\\n",
    "        + \"&sort\" + sort_type \\\n",
    "        + \"nso=so%3Ar%2Cp%3Afrom\" + start_date \\\n",
    "        + \"to\" + end_date \\\n",
    "        + \"%2Ca%3A&start=\" + str(current_call)\n",
    "        \n",
    "        urls_list = []\n",
    "        \n",
    "        try:\n",
    "            web = requests.get(url).content\n",
    "            source = BeautifulSoup(web,'html.parser')\n",
    "            for urls in source.find_all('a',{'class':'info'}):\n",
    "                if urls[\"href\"].startswith(\"https://n.news.naver.com\"):\n",
    "                    urls_list.append(urls[\"href\"])\n",
    "        except:\n",
    "            print(\"해당 뉴스 검색 페이지의 뉴스 링크를 모으는 중 에러 발생: \", url)\n",
    "        \n",
    "        \n",
    "        if urls_list != []:\n",
    "            for url in urls_list:\n",
    "                try:\n",
    "                    headers = {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.101 Safari/537.36where: newssm: tab_jumquery: ' + quote('데이터분석')}\n",
    "                    web_news = requests.get(url, headers=headers).content\n",
    "                    source_news = BeautifulSoup(web_news,'html.parser')\n",
    "                    # 2) 기사 제목\n",
    "                    title = source_news.find('h2', {'class':'media_end_head_headline'}).get_text()\n",
    "                    print('Processing article : {}'.format(title))\n",
    "\n",
    "                    # 3) 기사 날짜\n",
    "                    date = source_news.find('span',{'class':'media_end_head_info_datestamp_time'}).get_text()\n",
    "                    #print('Date of article : {}'.format(date))\n",
    "\n",
    "                    # 4) 기사 본문\n",
    "                    article = source_news.find('div',{'id':'dic_area'}).get_text()\n",
    "                    article = article.replace(\"\\n\",\"\")\n",
    "                    article = article.replace(\"\\t\",\"\")\n",
    "\n",
    "                    # 5) 기사 발행 언론사\n",
    "                    press_company = source_news.find('em',{'class':'media_end_linked_more_point'}).get_text()\n",
    "\n",
    "                    # 위 2~5를 통해 성공적으로 제목/날짜/본문/언론사 정보가 모두 추출되었을 때에만 리스트에 추가해 길이를 동일하게 유지\n",
    "                    titles.append(title)\n",
    "                    dates.append(date)\n",
    "                    articles.append(article)\n",
    "                    press_companies.append(press_company)\n",
    "                    article_urls.append(url)\n",
    "                except:\n",
    "                    print(\"다음 뉴스기사의 링크를 스크랩 중 오류가 발생하였음 : {}\".format(url))\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        time.sleep(5)\n",
    "        current_call += 10\n",
    "        \n",
    "    article_df = pd.DataFrame({'Title':titles,\n",
    "                            'Data':dates,\n",
    "                          'Article':articles,\n",
    "                          'URL':article_urls,\n",
    "                          'PressCompany':press_companies})\n",
    "    from openpyxl import Workbook\n",
    "    article_df.to_excel('result2_{}.xlsx'.format(datetime.now().strftime('%y%m%d_%H%M')),index=False,encoding='utf-8')\n",
    "    article_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "7a5f3ff7d4704552eb2caa29f2a36719b55c3eff8134ba0e462e2e872350caac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
